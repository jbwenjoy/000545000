{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ad3a4da",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jbwenjoy/000545000/blob/jbw/CIS_545_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cce5ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48d00197",
   "metadata": {},
   "source": [
    "# **CIS 5450 Final Project - Spring 2025**\n",
    "## Group Member:\n",
    "Yiyan Liang: edgarl@seas.upenn.edu\n",
    "<br>Bowen Jiang: jbwenjoy@seas.upenn.edu\n",
    "<br>Binglong Bao: binglong@seas.upenn.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4048df77",
   "metadata": {},
   "source": [
    "# **1. Introduction and Background**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76933488",
   "metadata": {},
   "source": [
    "## **1.1 Problem Statement**\n",
    "Airfare pricing in the U.S. domestic airline industry has long been a topic of interest due to its dynamic and opaque nature. From 1993 to 2024, the industry has undergone substantial changes including fluctuating fuel prices, shifting market competition, and evolving consumer behavior. These factors make it difficult for travelers to anticipate ticket prices and for airlines to optimize revenue through effective pricing strategies.\n",
    "\n",
    "Despite the abundance of large-scale data, there remains a lack of comprehensive, data-driven analyses that systematically explore the relationship between these variables and airfare trends. One of our team members is planning to book a flight during a holiday period and is particularly interested in understanding what drives changes in ticket prices. This curiosity sparked our interest and motivated us to investigate whether historical data could reveal meaningful patterns that help consumers make more informed travel decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d2c99b",
   "metadata": {},
   "source": [
    "## **1.2 Objective and Value Proposition**\n",
    "The objective of this project is to conduct a thorough exploratory data analysis (EDA) of U.S. domestic airline fares from 1993 to 2024, and to develop predictive models that capture how different variables affect airfare pricing. The project integrates route and fare data with external economic indicators such as oil prices to uncover insights into airfare dynamics.\n",
    "\n",
    "The primary goals are:\n",
    "\n",
    "- **Understand Key Drivers of Pricing:** Investigate how factors like route distance, seasonality, fuel costs, airline competition, and passenger volume affect airfares.\n",
    "\n",
    "- **Support Strategic Planning:** Provide insights to airline operators and policymakers for optimizing pricing strategies and understanding long-term market shifts.\n",
    "\n",
    "- **Build a Predictive Model:** Develop regression models capable of forecasting average fares given known market and economic conditions.\n",
    "\n",
    "The value proposition of this project lies in offering a systematic and scalable framework for analyzing complex fare-setting behavior across three decades. For consumers, it may lead to better fare predictions and smarter booking decisions. For industry stakeholders, it provides tools for data-informed pricing and long-term planning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290d20a6",
   "metadata": {},
   "source": [
    "# **2. Data Loading and Preprocessing**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78374f5a",
   "metadata": {},
   "source": [
    "## **2.1 ðŸ“Š Overview of the Dataset**\n",
    "This project uses two complementary datasets to explore the relationship between airline ticket pricing and external economic factors, particularly oil prices. The integration of these datasets allows for a data-driven analysis of historical fare trends in the U.S. domestic airline market over the past three decades.\n",
    "### **âœˆï¸ US Airline Flight Routes and Fares (1993â€“2024)**\n",
    "**Source**: [Kaggle Dataset â€“ US Airline Flight Routes and Fares (1993â€“2024)](https://www.kaggle.com/datasets/bhavikjikadara/us-airline-flight-routes-and-fares-1993-2024)\n",
    "\n",
    "This dataset provides detailed information on airline flight routes, average fares, passenger volume, and carrier competition across the United States.\n",
    "\n",
    "**Key Features:**\n",
    "- **Year & Quarter**: Time identifiers for each record, from 1993 to 2024.\n",
    "- **City1 & City2**: Names of origin and destination cities.\n",
    "- **Airport Codes**: Origin and destination airport identifiers.\n",
    "- **nsmiles**: Distance between airports in miles.\n",
    "- **Passengers**: Number of passengers on each route.\n",
    "- **Fare**: Average fare for the route.\n",
    "- **Carrier Info**: Largest and lowest-fare carrier codes, their market shares, and corresponding fares.\n",
    "- **Geographical Coordinates**: Latitude and longitude of both origin and destination cities.\n",
    "\n",
    "\n",
    "The dataset enables long-term trend analysis of pricing behavior, route dynamics, and airline competition in the domestic travel sector.\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ›¢ï¸ WTI Crude Oil Daily Prices**\n",
    "\n",
    "Sourced from [DataHub](https://datahub.io/core/oil-prices), this dataset provides historical daily spot prices for West Texas Intermediate (WTI) crude oil.\n",
    "\n",
    "**Key Features:**\n",
    "- **Date**: Daily timestamps for each price record.\n",
    "- **Price**: The WTI crude oil price in USD per barrel.\n",
    "\n",
    "This dataset supports economic analysis and helps model the impact of fuel prices on airline operating costs.  \n",
    "\n",
    "By combining historical airfare and fuel price data, this project aims to:\n",
    "- Uncover patterns in ticket pricing.\n",
    "- Investigate the impact of fuel cost fluctuations on airline fares.\n",
    "- Improve prediction accuracy using external economic indicators.\n",
    "\n",
    "This integrated approach allows for a richer understanding of pricing behavior and supports more informed consumer and industry decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e01b10",
   "metadata": {},
   "source": [
    "## **2.2 Data Loading**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5352b819",
   "metadata": {},
   "source": [
    "Import the relevant libraries for all stages of our report (pre-processing, exploratory data analysis, and model selection). These imports primarily span fundamental libraries such as `pandas`, `matplotlib`, `numpy`, and `sklearn`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47689537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures, FunctionTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6112c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to mount your drive (you will be prompted to sign in)\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf43609",
   "metadata": {},
   "source": [
    "â— ONLY RUN ONE OF THE TWO CELLS BELOW, SEE COMMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58171b4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RUN THIS CELL IF YOU WANT TO LOAD CACHED DATASETS FROM GOOGLE DRIVE\n",
    "DO NOT RUN THE NEXT CELL IF YOU DECIDE TO RUN THIS ONE!\n",
    "\"\"\"\n",
    "\n",
    "# If this is colab env, then mount the drive\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab')\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    data_path_in_drive = '/content/drive/MyDrive/CIS545-final/'\n",
    "    flights_data = '/content/drive/MyDrive/CIS545-final/US Airline Flight Routes and Fares 1993-2024.csv'\n",
    "    fuel_data = '/content/drive/MyDrive/CIS545-final/flat-ui__data-Fri Mar 28 2025.csv'\n",
    "    kaggle_key = '/content/drive/MyDrive/CIS545-final/kaggle.json'\n",
    "else:\n",
    "    print('Not running on CoLab')\n",
    "    data_path_in_drive = './'\n",
    "    flights_data = './US Airline Flight Routes and Fares 1993-2024.csv'\n",
    "    fuel_data = './flat-ui__data-Fri Mar 28 2025.csv'\n",
    "    kaggle_key = './kaggle.json'\n",
    "\n",
    "# Check if these files exists\n",
    "file_paths = [\n",
    "    (data_path_in_drive, \"Directory\"),\n",
    "    (flights_data, \"File\"),\n",
    "    (fuel_data, \"File\"),\n",
    "    (kaggle_key, \"File\")\n",
    "]\n",
    "file_exists = True\n",
    "for path, file_type in file_paths:\n",
    "    if not os.path.exists(path):\n",
    "        file_exists = False\n",
    "        if file_type == \"Directory\":\n",
    "            os.makedirs(path)\n",
    "            print(f\"Directory '{path}' created successfully.\")\n",
    "        else:\n",
    "            print(f\"{file_type} '{path}' does not exist.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"{file_type} '{path}' exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52959b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RUN THIS CELL IF YOU WANT TO DOWNLOAD ONLINE DATASETS FROM KAGGLE AND DATAHUB\n",
    "DO NOT RUN THE PREVIOUS CELL IF YOU DECIDE TO RUN THIS ONE!\n",
    "\"\"\"\n",
    "try:\n",
    "    file_exists\n",
    "except NameError:\n",
    "    file_exists = False\n",
    "if not file_exists:\n",
    "    # Create the kaggle directory and\n",
    "    # (NOTE: Do NOT run this cell more than once unless restarting kernel)\n",
    "    !mkdir ~/.kaggle\n",
    "\n",
    "    # Read the uploaded kaggle.json file\n",
    "    !cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
    "\n",
    "    # Download flights dataset (DO NOT CHANGE)\n",
    "    !kaggle datasets download -d bhavikjikadara/us-airline-flight-routes-and-fares-1993-2024\n",
    "    !unzip /content/us-airline-flight-routes-and-fares-1993-2024\n",
    "\n",
    "    flights_data = 'US Airline Flight Routes and Fares 1993-2024.csv'\n",
    "    flights_df = pd.read_csv(flights_data, low_memory=False)\n",
    "\n",
    "    # Download WTI dataset (DO NOT CHANGE)\n",
    "    url = \"https://datahub.io/core/oil-prices/r/wti-daily.csv\"\n",
    "    oil_df = pd.read_csv(url)\n",
    "    oil_df.to_csv(\"flat-ui__data-Fri Mar 28 2025.csv\", index=False)\n",
    "    fuel_data = 'flat-ui__data-Fri Mar 28 2025.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cf8585",
   "metadata": {},
   "source": [
    "## **2.3 Data Preprocessing**\n",
    "### ðŸ” Identified Data Challenges\n",
    "- **Missing and Anomalous Values**: Both datasets contain potential null values and outliers that need to be addressed for accurate analysis.\n",
    "- **Data Integration**: Airfare data is provided on a quarterly basis, while oil prices are reported daily. Proper aggregation and alignment are necessary to merge the two datasets effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ› ï¸ Proposed Solutions\n",
    "\n",
    "- **Outlier Detection and Filtering**: Use statistical thresholds to filter out unreasonable values (e.g., negative fares or extremely long flights).\n",
    "- **Handling Missing Values**: Apply appropriate methods such as row filtering, median imputation, or interpolation to manage missing entries.\n",
    "- **Quarter-Based Aggregation**: Convert daily oil price data into quarterly averages to match the structure of the airfare dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdada69e",
   "metadata": {},
   "source": [
    "### **2.3.1 Flight Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c1a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv(flights_data, low_memory=False)\n",
    "df_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b75b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Duplicates\n",
    "duplicates = df_original.duplicated().sum()\n",
    "print(f'Duplicates: {duplicates}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c80a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e182121",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d362c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3aaf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_original.shape)\n",
    "\n",
    "# Handling Missing Values\n",
    "df_clean = df_original.copy()\n",
    "missing_values = df_clean.isnull().sum()\n",
    "print(\"------MISS VALUES------\")\n",
    "print(missing_values.to_string())\n",
    "df_clean = df_clean.dropna()\n",
    "df_clean = df_clean.dropna(axis=1)\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "\n",
    "print(df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7ab07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3af15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the distribution of fare, to see if there are outliers\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df_clean['fare'], kde=True, color='#800080', edgecolor='white', linewidth=1.5)\n",
    "plt.title('Distribution of Fare', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Fare (USD)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)  # Added gridlines for better readability\n",
    "sns.despine()  # Remove top and right spines for a cleaner look\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424b091a",
   "metadata": {},
   "source": [
    "â— BE CAREFUL: YOU CAN ONLY RUN BELOW ONCE WITHOUT RERUNNING THE CELL ABOVE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ea797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YOU CAN ONLY RUN THIS CELL ONCE!\n",
    "OR YOU NEED TO RERUN THE CELL ABOVE THIS\n",
    "\"\"\"\n",
    "\n",
    "# Calculate Z-scores of each value in the DataFrame\n",
    "z_scores = stats.zscore(df_clean.select_dtypes(include=['float64', 'int64']))\n",
    "\n",
    "# Identify outliers\n",
    "outlier_score = 3\n",
    "df_outliers = df_clean[(z_scores > outlier_score).any(axis=1) | (z_scores < -outlier_score).any(axis=1)]\n",
    "print(df_outliers.shape)\n",
    "\n",
    "# Remove rows with outliers based on Z-score\n",
    "df_clean = df_clean[(z_scores < outlier_score).all(axis=1) & (z_scores > -outlier_score).all(axis=1)]\n",
    "print(df_clean.shape)\n",
    "df_clean.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f216c60a",
   "metadata": {},
   "source": [
    "### **2.3.2 Fuel Price Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b5578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_df_original = pd.read_csv(fuel_data, low_memory=False)\n",
    "fuel_df_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Duplicates\n",
    "duplicates = fuel_df_original.duplicated().sum()\n",
    "print(f'Duplicates: {duplicates}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81404cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_df_original.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562ca16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fuel_df_original.shape)\n",
    "\n",
    "# Handling Missing Values\n",
    "fuel_df_clean = fuel_df_original.copy()\n",
    "fuel_df_clean = fuel_df_clean.dropna()\n",
    "fuel_df_clean = fuel_df_clean.dropna(axis=1)\n",
    "fuel_df_clean = fuel_df_clean.drop_duplicates()\n",
    "\n",
    "print(fuel_df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6170314",
   "metadata": {},
   "source": [
    "Since the fuel price data is very clean, we don't need further cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a24e6b0",
   "metadata": {},
   "source": [
    "# **3. Exploratory Data Analysis (EDA)**\n",
    "\n",
    "We primarily focused on visualizing data distributions to have a brief concept of the data ranges, and also the relationships between fare and possible deciding factors (fuel price, distance, passenger count, carrier type, etc) to understand possible correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b42badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_clean.copy()\n",
    "fuel_df = fuel_df_clean.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7046cda",
   "metadata": {},
   "source": [
    "## **3.1 Distribution of Fare, distance and fuel price**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8683efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of fare\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df['fare'], kde=True, color='#800080', edgecolor='white', linewidth=1.5)\n",
    "plt.title('Distribution of Fare', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Fare (USD)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)  # Added gridlines for better readability\n",
    "sns.despine()  # Remove top and right spines for a cleaner look\n",
    "plt.show()\n",
    "\n",
    "# Distribution of distance\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df['nsmiles'], kde=True, color='#000080', edgecolor='white', linewidth=1.5)\n",
    "plt.title('Distribution of Distance', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Distance (miles)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "sns.despine()\n",
    "plt.show()\n",
    "\n",
    "# Distribution of fuel price\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(fuel_df['Price'], kde=True, color='#008080', edgecolor='white', linewidth=1.5)\n",
    "plt.title('Distribution of Fuel Price', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Fuel Price (USD)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa123d7b",
   "metadata": {},
   "source": [
    "We can see that:\n",
    "\n",
    "* Airfare centers around 200 USD, while there are extreme values, it's still very symmetrical.\n",
    "\n",
    "* Distance range from about 200 miles to over 2500 miles, with 1500 miles serving as a dividing line. The distribution is fairly uniform both below and above 1,500 miles, but the average frequency in these two segments differs by about a factor of two.\n",
    "\n",
    "* Fuel price ranges greatly over the years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c536e8",
   "metadata": {},
   "source": [
    "## **3.2 US Flight Fare and Distance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c554abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(x='nsmiles', y='fare', data=df, alpha=0.1, color='royalblue', edgecolor='k', s=20)\n",
    "plt.title('Relationship between Fare and Distance', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Miles', fontsize=14)\n",
    "plt.ylabel('Fare (USD)', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302c64ff",
   "metadata": {},
   "source": [
    "From the plot:\n",
    "\n",
    "* There is an approximately linear relationship between airfare and the distance of the route.\n",
    "\n",
    "* The slope is small and the variance is very large. This aligns with our real-world experience that airfare can vary greatly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06334168",
   "metadata": {},
   "source": [
    "## **3.3 US Flight Fare and Fuel Price over Time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d5256",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_df['Date'] = pd.to_datetime(fuel_df['Date'])\n",
    "fuel_df['Year'] = fuel_df['Date'].dt.year\n",
    "avg_fuel_price_per_year = fuel_df.groupby('Year')['Price'].mean().reset_index()\n",
    "avg_fare_per_year = df.groupby('Year')['fare'].mean().reset_index()\n",
    "\n",
    "# Find common years between fare and fuel data\n",
    "common_years = list(set(avg_fare_per_year['Year']) & set(avg_fuel_price_per_year['Year']))\n",
    "\n",
    "# Filter data for common years\n",
    "avg_fare_per_year = avg_fare_per_year[avg_fare_per_year['Year'].isin(common_years)]\n",
    "avg_fuel_price_per_year = avg_fuel_price_per_year[avg_fuel_price_per_year['Year'].isin(common_years)]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Avr fare (ax1)\n",
    "sns.lineplot(x='Year', y='fare', data=avg_fare_per_year, marker='o', color='blue', ax=ax1, label='Average Fare')\n",
    "ax1.set_xlabel('Year', fontsize=12)\n",
    "ax1.set_ylabel('Average Fare (USD)', color='blue', fontsize=12)\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Avr fuel price (ax2)\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(x='Year', y='Price', data=avg_fuel_price_per_year, marker='s', color='red', ax=ax2, label='Average Fuel Price')\n",
    "ax2.set_ylabel('Average Fuel Price (USD)', color='red', fontsize=12)\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "ax2.spines['right'].set_visible(True)\n",
    "ax2.spines['right'].set_color('red')\n",
    "plt.title('Average Fare and Fuel Price Over Time (by Year)', fontsize=14, fontweight='bold')\n",
    "# fig.legend(loc=\"upper left\", bbox_to_anchor=(0.1, 0.95))\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8271135a",
   "metadata": {},
   "source": [
    "From the plot:\n",
    "\n",
    "* We can see some correlation between flight ticket fare and fuel price, but averging by year is somewhat rough.\n",
    "\n",
    "* Below we also plot base on quarterly average, and we can see that the fluctuations of the two are more obvious.\n",
    "\n",
    "* Airfare tend to change a little bit later/slower than the fuel price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e306cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuel_df['Date'] = pd.to_datetime(fuel_df['Date'])\n",
    "fuel_df['Year'] = fuel_df['Date'].dt.year\n",
    "fuel_df['Quarter'] = fuel_df['Date'].dt.quarter\n",
    "avg_fuel_price_per_quarter = fuel_df.groupby(['Year', 'Quarter'])['Price'].mean().reset_index()\n",
    "\n",
    "avg_fare_per_quarter = df.groupby(['Year', 'quarter'])['fare'].mean().reset_index()\n",
    "avg_fare_per_quarter = avg_fare_per_quarter.rename(columns={'quarter': 'Quarter'})\n",
    "\n",
    "# Merge and filter data for common quarters\n",
    "merged_df = pd.merge(avg_fare_per_quarter, avg_fuel_price_per_quarter, on=['Year', 'Quarter'], how='inner')\n",
    "merged_df['Year-Quarter'] = merged_df['Year'].astype(str) + '-Q' + merged_df['Quarter'].astype(str)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Avr flight fare (ax1)\n",
    "sns.lineplot(x='Year-Quarter', y='fare', data=merged_df, marker='o', color='blue', ax=ax1, label='Average Fare')\n",
    "ax1.set_xlabel('Year-Quarter', fontsize=12)\n",
    "ax1.set_ylabel('Average Fare (USD)', color='blue', fontsize=12)\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Avr fuel price (ax2)\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(x='Year-Quarter', y='Price', data=merged_df, marker='s', color='red', ax=ax2, label='Average Fuel Price')\n",
    "ax2.set_ylabel('Average Fuel Price (USD)', color='red', fontsize=12)\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Add solid line to the right spine of ax2\n",
    "ax2.spines['right'].set_visible(True)\n",
    "ax2.spines['right'].set_color('red')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.title('Average Quarterly Fare and Fuel Price Over Time (by Season)', fontsize=14, fontweight='bold')\n",
    "# fig.legend(loc=\"upper left\", bbox_to_anchor=(0.1, 0.95))\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "ax1.xaxis.set_major_locator(plt.MaxNLocator(15)) # Maximum num of x labels\n",
    "\n",
    "# Improve aesthetics\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7bc404",
   "metadata": {},
   "source": [
    "## **3.4 US Flight Fare and Passenger Count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeffc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_passengers_per_quarter = df.groupby(['Year', 'quarter'])['passengers'].mean().reset_index()\n",
    "avg_fare_per_quarter = df.groupby(['Year', 'quarter'])['fare'].mean().reset_index()\n",
    "\n",
    "avg_passengers_per_quarter = avg_passengers_per_quarter.rename(columns={'quarter': 'Quarter'})\n",
    "avg_fare_per_quarter = avg_fare_per_quarter.rename(columns={'quarter': 'Quarter'})\n",
    "\n",
    "merged_df = pd.merge(avg_passengers_per_quarter, avg_fare_per_quarter, on=['Year', 'Quarter'], how='inner')\n",
    "merged_df['Year-Quarter'] = merged_df['Year'].astype(str) + '-Q' + merged_df['Quarter'].astype(str)\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(15, 6))\n",
    "sns.lineplot(x='Year-Quarter', y='fare', data=merged_df, marker='s', color='blue', ax=ax1, label='Average Fare')\n",
    "ax1.set_ylabel('Average Fare (USD)', color='blue', fontsize=12)\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(x='Year-Quarter', y='passengers', data=merged_df, marker='o', color='red', ax=ax2, label='Average Passengers')\n",
    "ax2.set_xlabel('Year-Quarter', fontsize=12)\n",
    "ax2.set_ylabel('Average Passengers', color='red', fontsize=12)\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "ax2.spines['right'].set_visible(True)\n",
    "ax2.spines['right'].set_color('red')\n",
    "\n",
    "plt.title('Average Quarterly Passengers and Fare Over Time', fontsize=14, fontweight='bold')\n",
    "# fig.legend(loc=\"upper left\", bbox_to_anchor=(0.1, 0.95))\n",
    "plt.xticks(rotation=45)\n",
    "ax1.xaxis.set_major_locator(plt.MaxNLocator(15))\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b32af4",
   "metadata": {},
   "source": [
    "From the plot:\n",
    "\n",
    "* Looks like there isn't a strong correlation between flight fare and passenger counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490bf69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(x='passengers', y='fare', data=df, alpha=0.1, color='royalblue', edgecolor='k', s=20)\n",
    "plt.title('Relationship between Fare (not averaged) and Passenger Count', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Passenger Count', fontsize=12)\n",
    "plt.ylabel('Fare (USD)', fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e45069a",
   "metadata": {},
   "source": [
    "From the plot:\n",
    "\n",
    "* We can see that the fares are more stable when there is a high passenger volume on a route."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39980d69",
   "metadata": {},
   "source": [
    "## **3.5 US Flight Fare and Carrier Type for Same Routes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c940e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(x='fare_low', y='fare_lg', data=df, alpha=0.1, color='royalblue', edgecolor='k', s=20)\n",
    "plt.title('Large and Low Carrier Fares for the Same Routes', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Fare of Low Carrier (USD)', fontsize=14)\n",
    "plt.ylabel('Fare of Large Carrier (USD)', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe85cded",
   "metadata": {},
   "source": [
    "From this plot:\n",
    "\n",
    "* We can roughly see that for air tickets between about 100-300 USD, the fares of different carriers vary greatly.\n",
    "\n",
    "* The differences become much smaller after 300 USD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799ed95",
   "metadata": {},
   "source": [
    "## **3.6 Top 10 Busiest Cities (Dept/Dest)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f435064",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Top 10 Busiest US Cities by Departing Traffic (1993-2024)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_cities = df.groupby('city1')['passengers'].sum().nlargest(10).reset_index()\n",
    "\n",
    "sns.barplot(x='passengers', y='city1', data=top_cities,\n",
    "            palette=\"viridis\",  # Use a color palette\n",
    "            edgecolor=\"black\",  # Add black border to bars\n",
    "            linewidth=1)  # Set border line width\n",
    "\n",
    "plt.title('Top 10 Busiest US Cities by Departing Traffic (1993-2024)',\n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Total Passengers (in Millions)', fontsize=14)\n",
    "plt.ylabel('Origin City', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafdad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Top 10 Busiest US Cities by Arriving Traffic (1993-2024)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_cities = df.groupby('city2')['passengers'].sum().nlargest(10).reset_index()\n",
    "\n",
    "sns.barplot(x='passengers', y='city2', data=top_cities,\n",
    "            palette=\"viridis\",\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=1)\n",
    "\n",
    "plt.title('Top 10 Busiest US Cities by Arriving Traffic (1993-2024)',\n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Total Passengers (in Millions)', fontsize=14)\n",
    "plt.ylabel('Origin City', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1194510a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "## Top 10 Busiest US Cities by Total Passenger Traffic (1993-2024)\n",
    "\n",
    "city_passengers = pd.concat([df.groupby('city1')['passengers'].sum(),\n",
    "                             df.groupby('city2')['passengers'].sum()]) \\\n",
    "                   .groupby(level=0).sum() \\\n",
    "                   .sort_values(ascending=False) \\\n",
    "                   .reset_index()\n",
    "\n",
    "top_cities = city_passengers.head(10)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='passengers', y='index', data=top_cities,\n",
    "            palette=\"viridis\", edgecolor=\"black\", linewidth=1)\n",
    "\n",
    "plt.title('Top 10 Busiest US Cities by Total Passenger Traffic (1993-2024)',\n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Total Passengers (in Millions)', fontsize=14)\n",
    "plt.ylabel('City', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504b191b",
   "metadata": {},
   "source": [
    "## **3.7 Correlation Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5073b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_cols(dataframe, cat_th=10, car_th=20):\n",
    "    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes in ['object' , 'category' , 'bool']]\n",
    "    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and dataframe[col].dtypes in [\"int64\", \"float64\"]]\n",
    "    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and str(dataframe[col].dtypes) in [\"category\", \"object\"]]\n",
    "    cat_cols = cat_cols + num_but_cat\n",
    "    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n",
    "\n",
    "    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes in [\"int64\", \"float64\"]]\n",
    "    num_cols = [col for col in num_cols if col not in cat_cols]\n",
    "    return num_cols\n",
    "\n",
    "\n",
    "num_cols = get_numerical_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85106e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list_for_corr = num_cols + [\"quarter\"]\n",
    "\n",
    "corr_matrix = df[col_list_for_corr].corr(numeric_only=True)\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr_matrix, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True, fmt=\".2f\", annot_kws={\"size\": 10})\n",
    "\n",
    "plt.title('Correlation Matrix of Flight Fare Data', fontsize=16, fontweight='bold')\n",
    "plt.xticks(fontsize=12, rotation=45, ha='right')\n",
    "plt.yticks(fontsize=12)\n",
    "ax.tick_params(axis='both', which='major', pad=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb7161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additionally, calculate the correlation between airfare and fuel price\n",
    "\n",
    "fuel_df['Date'] = pd.to_datetime(fuel_df['Date'])\n",
    "fuel_df['Year'] = fuel_df['Date'].dt.year\n",
    "fuel_df['Quarter'] = fuel_df['Date'].dt.quarter\n",
    "avg_fuel_price_per_quarter = fuel_df.groupby(['Year', 'Quarter'])['Price'].mean().reset_index()\n",
    "\n",
    "avg_fare_per_quarter = df.groupby(['Year', 'quarter'])['fare'].mean().reset_index()\n",
    "avg_fare_per_quarter = avg_fare_per_quarter.rename(columns={'quarter': 'Quarter'})\n",
    "\n",
    "# Merge and filter data for common quarters\n",
    "merged_df = pd.merge(avg_fare_per_quarter, avg_fuel_price_per_quarter, on=['Year', 'Quarter'], how='inner')\n",
    "merged_df['Year-Quarter'] = merged_df['Year'].astype(str) + '-Q' + merged_df['Quarter'].astype(str)\n",
    "\n",
    "correlation_fuel_fare = merged_df['fare'].corr(merged_df['Price'])\n",
    "print(f\"Correlation between airfare and fuel price (quarterly average): {correlation_fuel_fare:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916f2147",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e04be1cd",
   "metadata": {},
   "source": [
    "From the correlation analysis above, we can see that airfare does have some correlation with existing numerical features. Higher fuel price, longer distance, and less passengers tend to lead to higher fare price. Large carriers tend to dominate the market and affect the airfare the most, low-fare carriers can have less market share and more flexible pricing policy and thus lead to a weaker correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558b612e",
   "metadata": {},
   "source": [
    "# **4. Feature Engineering**\n",
    "\n",
    "To improve model performance and enhance feature interpretability, we applied the following feature engineering steps:\n",
    "\n",
    "- **Categorical Encoding**: We identified `quarter`, `citymarketid_1`, and `citymarketid_2` as categorical variables, despite being numeric in appearance. These, along with other nominal features such as `city1`, `airport_1`, and `carrier_lg`, were one-hot encoded to convert them into a format suitable for machine learning models.\n",
    "- **Target Variable**: We selected `fare` as the prediction target, representing the average airfare for each route.\n",
    "- **Train-Test Split**: The dataset was split into 80% for training and 20% for testing, ensuring unbiased model evaluation.\n",
    "- **Feature Scaling**: Continuous numerical features were standardized using `StandardScaler` to ensure they are on the same scale, which benefits many machine learning algorithms.\n",
    "- **Dimensionality Reduction**: PCA was applied to retain 95% of the variance while reducing the dimensionality of the feature space, helping to speed up model training and reduce the risk of overfitting.\n",
    "\n",
    "This structured pipeline ensures that each feature is treated appropriately according to its type, resulting in a clean and model-ready dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434cd1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature = df_clean.copy()\n",
    "print(df_feature.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e16109",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "### Revised Feature Engineering (Based on Feature Type Analysis)\n",
    "\n",
    "# Step 1: Copy the cleaned dataset\n",
    "df_feature = df_clean.copy()\n",
    "\n",
    "# Step 2: Identify categorical features\n",
    "categorical_cols = [\n",
    "    'quarter',\n",
    "    'citymarketid_1',\n",
    "    'citymarketid_2',\n",
    "    'city1', 'city2',\n",
    "    'airport_1', 'airport_2',\n",
    "    'carrier_lg', 'carrier_low',\n",
    "    'Geocoded_City1', 'Geocoded_City2'\n",
    "]\n",
    "\n",
    "\n",
    "categorical_cols = [col for col in categorical_cols if col in df_feature.columns]\n",
    "\n",
    "# Apply One-Hot Encoding to categorical columns\n",
    "df_feature = pd.get_dummies(df_feature, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Step 3: Define target variable\n",
    "target_col = 'fare'\n",
    "X = df_feature.drop(columns=[target_col])\n",
    "y = df_feature[target_col]\n",
    "\n",
    "# Step 4: Train-Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Standardize only numeric columns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select only numeric columns for scaling\n",
    "numeric_cols = X_train.select_dtypes(include=['number']).columns\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test_scaled = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "# Step 6: Optional - PCA to reduce dimensionality\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)  # Retain 95% variance\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(f\"Original number of numeric features: {X_train.shape[1]}\")\n",
    "print(f\"After PCA: {X_train_pca.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c1dec5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select a few numeric features to visualize (to keep the plot clean)\n",
    "selected_cols = numeric_cols[:4]  # Adjust the number as needed\n",
    "\n",
    "# Create subplots: 2 rows (before/after), N columns (features)\n",
    "fig, axes = plt.subplots(2, len(selected_cols), figsize=(16, 8))\n",
    "fig.suptitle('Distribution Before vs After Standardization', fontsize=16)\n",
    "\n",
    "for i, col in enumerate(selected_cols):\n",
    "    # Plot original feature distribution (row 0)\n",
    "    sns.histplot(X_train[col], bins=30, kde=True, ax=axes[0, i], color='skyblue')\n",
    "    axes[0, i].set_title(f'Original: {col}')\n",
    "\n",
    "    # Plot standardized feature distribution (row 1)\n",
    "    col_index = list(numeric_cols).index(col)\n",
    "    sns.histplot(X_train_scaled[:, col_index], bins=30, kde=True, ax=axes[1, i], color='salmon')\n",
    "    axes[1, i].set_title(f'Scaled: {col}')\n",
    "\n",
    "# Adjust spacing\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01345cc1",
   "metadata": {},
   "source": [
    "### Feature Engineering Summary\n",
    "\n",
    "- We treated the following as **categorical variables** and applied One-Hot Encoding: `quarter`, `citymarketid_1`, `citymarketid_2`, and all text-based features such as city names, airports, and carriers.\n",
    "- Continuous numerical variables such as `Year`, `nsmiles`, `passengers`, etc. were retained and **standardized using StandardScaler**.\n",
    "- The dataset was then split into training and testing subsets (80/20).\n",
    "- Finally, **Principal Component Analysis (PCA)** was applied to reduce dimensionality while preserving 95% of the data variance.\n",
    "\n",
    "This setup ensures that all variables are in a format suitable for downstream modeling and that feature types are handled appropriately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b439caef",
   "metadata": {},
   "source": [
    "# **5. Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f69b389",
   "metadata": {},
   "source": [
    "## **5.1 Baseline model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaa124e",
   "metadata": {},
   "source": [
    "Originally, we utilize the linear regression as the baseline model. This regression model takes the distance ( <i>nmiles</i> ), time ( <i>Year</i>, <i>Quarter</i> ) and fuel price into consideration, and the target of prediction is exactly the average price of the given flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fabd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.base import TransformerMixin, BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6261ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_clean.copy()\n",
    "# df['quaters'] = df['quarter'].astype(int) + (df['Year'].astype(int) - 2000) * 4\n",
    "# add the oil price to df\n",
    "new_df = df[['fare', 'nsmiles', 'quarter', 'Year']].dropna()\n",
    "fuel_df = fuel_df_clean.copy()\n",
    "fuel_df_year_quarter = fuel_df.copy()\n",
    "fuel_df_year_quarter['Year'] = fuel_df_year_quarter['Date'].str[:4].astype(int)\n",
    "fuel_df_year_quarter['Quarter'] = fuel_df_year_quarter['Date'].str[5:7].astype(int)\n",
    "fuel_df_year_quarter['Quarter'] = (fuel_df_year_quarter['Quarter'] - 1) // 3 + 1\n",
    "fuel_df_year_quarter = fuel_df_year_quarter.drop('Date', axis=1)\n",
    "# find the mean oil price for the quarter\n",
    "fuel_df_year_quarter = fuel_df_year_quarter.groupby(['Year', 'Quarter']).mean().reset_index()\n",
    "\n",
    "new_df = pd.merge(new_df, fuel_df_year_quarter, left_on=['Year', 'quarter'], right_on=['Year', 'Quarter'], how='left')\n",
    "new_df = new_df.drop(['Quarter'], axis=1)\n",
    "\n",
    "numerical_features = ['Year', 'Price', 'nsmiles']\n",
    "categorical_features = ['quarter']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numerical_features),\n",
    "        (\"cat\", OneHotEncoder(drop='first'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X = new_df[['nsmiles', 'quarter', 'Year', 'Price']]\n",
    "y = new_df['fare']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e45350",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"pre\", preprocessor),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "print(f\"R^2 socre: {pipe.score(X_test, y_test)}\")\n",
    "print(f\"MSE score: {mean_squared_error(y_test, pipe.predict(X_test))}\")\n",
    "print(f\"model intercept: {pipe.named_steps['model'].intercept_}\")\n",
    "print(f\"model coefficients: {pipe.named_steps['model'].coef_}\")\n",
    "\n",
    "def plot_prediction(model, X_test, y_test):\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    # use plotly to plot y_train_pred and y_test_pred vs y_train and y_test\n",
    "    fig = go.Figure()\n",
    "    # control figure size\n",
    "    fig.update_layout(width=800, height=800)\n",
    "    fig.add_trace(go.Scatter(x=y_test, y=y_test_pred, mode='markers', name='test'))\n",
    "    min_val = min(y_test.min(), y_test_pred.min())\n",
    "    max_val = max(y_test.max(), y_test_pred.max())\n",
    "    fig.update_xaxes(range=[.75 * min_val, 1.25 * max_val])\n",
    "    fig.update_yaxes(range=[.75 * min_val, 1.25 * max_val])\n",
    "    # add title and axis labels\n",
    "    fig.update_layout(title='Prediction vs Actual',\n",
    "                    xaxis_title='Actual',\n",
    "                    yaxis_title='Prediction')\n",
    "    fig.add_trace(go.Scatter(\n",
    "                    x=[min_val, max_val],\n",
    "                    y=[min_val, max_val],\n",
    "                    mode='lines',\n",
    "                    name='Ideal (y = x)',\n",
    "                    line=dict(color='red', dash='dash')\n",
    "    ))\n",
    "    fig.show()\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "# sample some points from X_test and y_test\n",
    "X_test_sample = X_test.sample(100)\n",
    "y_test_sample = y_test.loc[X_test_sample.index]\n",
    "# plot the prediction\n",
    "plot_prediction(pipe, X_test_sample, y_test_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e93432",
   "metadata": {},
   "source": [
    "The performance for the baseline model is not quite good, which shows that merely linear relationship can hardly justify the fare price given the parameters. We will try to optimize it via adding some new features and employing more advanced models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2856813d",
   "metadata": {},
   "source": [
    "## **5.2 Ridge Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d2f312",
   "metadata": {},
   "source": [
    "We first try to utilize ridge regression to improve the performance of the linear regression. Keeping all the conditions invariant, we employ ridge regression like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f890488",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_pipe = Pipeline([\n",
    "    (\"pre\", preprocessor),\n",
    "    ('model', Ridge(alpha=10.0)),\n",
    "])\n",
    "\n",
    "ridge_pipe.fit(X_train, y_train)\n",
    "print(f\"R^2 socre: {ridge_pipe.score(X_test, y_test)}\")\n",
    "print(f\"MSE socre: {mean_squared_error(y_test, ridge_pipe.predict(X_test))}\")\n",
    "print(f\"model intercept: {ridge_pipe.named_steps['model'].intercept_}\")\n",
    "print(f\"model coefficients: {ridge_pipe.named_steps['model'].coef_}\")\n",
    "\n",
    "# plot the prediction\n",
    "X_test_sample = X_test.sample(100)\n",
    "y_test_sample = y_test.loc[X_test_sample.index]\n",
    "plot_prediction(ridge_pipe, X_test_sample, y_test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d4f547",
   "metadata": {},
   "source": [
    "We also could try to utlize some polynomial features to increase the performance of the regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6460f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some more features\n",
    "new_df['nsmiles_squared'] = new_df['nsmiles'] ** 2\n",
    "new_df['price_squared'] = new_df['Price'] ** 2\n",
    "new_df['nsmiles_times_price'] = new_df['Price'] * new_df['nsmiles']\n",
    "\n",
    "X = new_df[['nsmiles', 'quarter', 'Year', 'Price', 'nsmiles_squared', 'price_squared', 'nsmiles_times_price']]\n",
    "Y = new_df['fare']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "new_numerical_features = ['Year', 'Price', 'nsmiles', 'nsmiles_squared', 'price_squared', 'nsmiles_times_price']\n",
    "new_categorical_features = ['quarter']\n",
    "\n",
    "new_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), new_numerical_features),\n",
    "        (\"cat\", OneHotEncoder(drop='first'), new_categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "new_ridge_pipe = Pipeline([\n",
    "    (\"pre\", new_preprocessor),\n",
    "    ('model', Ridge(alpha=10.0)),\n",
    "])\n",
    "\n",
    "new_ridge_pipe.fit(X_train, y_train)\n",
    "print(f\"R^2 socre: {new_ridge_pipe.score(X_test, y_test)}\")\n",
    "print(f\"MSE socre: {mean_squared_error(y_test, new_ridge_pipe.predict(X_test))}\")\n",
    "print(f\"model intercept: {new_ridge_pipe.named_steps['model'].intercept_}\")\n",
    "print(f\"model coefficients: {new_ridge_pipe.named_steps['model'].coef_}\")\n",
    "\n",
    "X_test_sample = X_test.sample(100)\n",
    "y_test_sample = y_test.loc[X_test_sample.index]\n",
    "plot_prediction(new_ridge_pipe, X_test_sample, y_test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7c5dd7",
   "metadata": {},
   "source": [
    "Utilizing the grid search technique to find the best parameter on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5211ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilize grid search\n",
    "param_grid = {\n",
    "    'model__alpha': [0.01, 0.1, 1, 5, 10, 100]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(new_ridge_pipe, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best param: \", grid.best_params_)\n",
    "print(\"CV RÂ² : \", grid.best_score_)\n",
    "print(\"Test RÂ² : \", grid.score(X_test, y_test))\n",
    "print(\"Test MSE: \", mean_squared_error(y_test, grid.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff7dd6d",
   "metadata": {},
   "source": [
    "Ridge regression only improve the model performance to the slightest level, showing that linear model could hardly address the prediction. Hence, more powerful non-linear models such as tree models and neural network could possibly help us to improve the performance of the model. Considering there are so many high dimensional categorical features, we think tree model like GBDT and XGBoost could be the best solution to such a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c9392",
   "metadata": {},
   "source": [
    "## **5.3 Xgboost**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34903b52",
   "metadata": {},
   "source": [
    "Linear model are not quite powerful when we have a lot of categorical data (i.e. the airport code and the name of the starting city). Also, linear model is not really good at dealing with multi-linear condition, this means that if the variables are highly correlated, the linear model may be ill-performed. Avoiding this case, we believe tree models like **Xgboost** is also a good way to address such a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491ab832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# involve more categorical data to make regression on Xgboost\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = df_clean.copy()\n",
    "\n",
    "tree_df = df[['fare', 'nsmiles', 'quarter', 'Year', 'city1', 'city2', 'passengers']].dropna()\n",
    "tree_df = pd.merge(tree_df, fuel_df_year_quarter, left_on=['Year', 'quarter'], right_on=['Year', 'Quarter'], how='left')\n",
    "tree_df = tree_df.drop(['Quarter'], axis=1)\n",
    "\n",
    "numerical_features = ['Year', 'Price', 'nsmiles']\n",
    "categorical_features = ['quarter', 'city1', 'city2']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numerical_features),\n",
    "        (\"cat\", OneHotEncoder(drop='first'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X = tree_df[['nsmiles', 'quarter', 'Year', 'Price', 'city1', 'city2']]\n",
    "X['quarter'] = X['quarter'].astype('category')\n",
    "X['city1'] = X['city1'].astype('category')\n",
    "X['city2'] = X['city2'].astype('category')\n",
    "\n",
    "y = tree_df['fare']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "tree_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    enable_categorical=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = tree_model.predict(X_test)\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test MSE: {mse:.4f}\")\n",
    "print(f\"Test R^2: {tree_model.score(X_test, y_test):.4f}\")\n",
    "\n",
    "X_test_sample = X_test.sample(100)\n",
    "y_test_sample = y_test.loc[X_test_sample.index]\n",
    "plot_prediction(tree_model, X_test_sample, y_test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8628e8a8",
   "metadata": {},
   "source": [
    "We could find there is a huge improvement when we try to adopt the tree model for regression, and we could do grid search to find the best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af8e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42,\n",
    "    enable_categorical=True\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=3, verbose=1, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "print(f\"Test MSE: {mean_squared_error(y_test, y_pred)}\")\n",
    "print(f\"Test R^2: {best_model.score(X_test, y_test)}\")\n",
    "\n",
    "plot_prediction(best_model, X_test_sample, y_test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c6cfc",
   "metadata": {},
   "source": [
    "As we can see, employing the ensembled tree model like XGBoost could significantly increase the $R^2$ and decrease the MSE loss. Via grid search, we could also do the hyperparameter tuning on the train set, so that we could find the best hyperparameter on the test set and real world data. However, since the flight pricing model is a complicated and time-variant model which is the most top secret for the airlines, simply market data could hardly justify the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f5116b",
   "metadata": {},
   "source": [
    "# **6. Project Management**\n",
    "### ðŸ—“ï¸ Timeline & Milestones\n",
    "| ðŸ“Œ Task                                | ðŸ—“ï¸ Deadline | âœ… Status     | ðŸ’¬ Notes |\n",
    "|----------------------------------------|-------------|---------------|---------|\n",
    "| Data acquisition and initial cleaning  | April 5     | âœ… Completed  | All raw data sources merged and cleaned |\n",
    "| Exploratory Data Analysis (EDA)        | April 10    | âœ… Completed  | Key visuals and early insights generated |\n",
    "| Baseline regression model              | April 12    | âœ… Completed  | Linear regression used as a baseline |\n",
    "| Advanced model training (RF, XGBoost)  | April 20    | ðŸŸ¡ In progress| Random Forest currently being tuned |\n",
    "| Model evaluation and result visualization | April 25 | â³ Upcoming   | Will compare models using RÂ² and RMSE |\n",
    "| Final report writing and presentation  | April 30    | â³ Upcoming   | Weâ€™ll prepare slides, summary, and submit final deliverables |\n",
    "\n",
    "### ðŸ“ˆ Progress Tracking\n",
    "\n",
    "We are using a shared Google Colab and GitHub repository to:\n",
    "- Track task completion status\n",
    "- Share code updates and experiment logs\n",
    "- Collaboratively edit the final presentation and written report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3428dc0",
   "metadata": {},
   "source": [
    "# **7. Hypothesis Testing**\n",
    "To further validate our insights, we plan to conduct statistical hypothesis testing on key factors that may influence airfare pricing. The following hypotheses are proposed:\n",
    "\n",
    "1. **Competition Hypothesis**  \n",
    "   - **Null Hypothesis (Hâ‚€):** The level of market competition on a route has no significant correlation with average airfare.  \n",
    "   - **Testing Method:** Correlation analysis and significance testing of regression coefficients will be used to evaluate the impact of competition intensity on pricing.\n",
    "\n",
    "2. **Seasonality Hypothesis**  \n",
    "   - **Null Hypothesis (Hâ‚€):** Seasonal variations in airfare do not differ significantly across routes with varying travel distances.  \n",
    "   - **Testing Method:** Two-way ANOVA (Analysis of Variance) will be applied to examine the interaction effect between seasonality and route distance categories on airfare.\n",
    "\n",
    "3. **Fuel Price Impact Hypothesis**  \n",
    "   - **Null Hypothesis (Hâ‚€):** The influence of fuel price fluctuations on airfare does not vary significantly across different types of flight routes (e.g., short-haul vs. long-haul).  \n",
    "   - **Testing Method:** Panel data regression analysis will be used to assess differential impacts of fuel price changes across route types over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111c7dec",
   "metadata": {},
   "source": [
    "# **8. Difficulty & Challenge**\n",
    "todo ..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c006108",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "source": [
    "# **9. Conclusion & Future work**\n",
    "todo ..\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
